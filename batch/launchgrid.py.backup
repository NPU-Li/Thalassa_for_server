#!/usr/bin/env python3
"""
LAUNCH BATCH PROPAGATIONS OVER A GRID OF INITIAL CONDITIONS
Modified version using mpi4py for multi-node parallel execution.

Author:
  Davide Amato (original)
  Modified for MPI multi-node support

Revisions:
  180529: Script creation.
  2024xx: Modified for MPI multi-node parallel execution.
"""

import argparse
import sys
import os
import json
import time
import datetime
import subprocess
import platform
import socket
from coegrid import chunkSize

# MPI imports
from mpi4py import MPI

thalassaPath = os.path.abspath('../thalassa.x')

def sizeof_fmt(num, suffix='B'):
    """
    Copied from https://web.archive.org/web/20111010015624/http://blogmag.net/
    blog/read/38/Print_human_readable_file_size .

    Author:
      Fred Cirera
    """
    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:
        if abs(num) < 1024.0:
            return "%3.1f%s%s" % (num, unit, suffix)
        num /= 1024.0
    
    return "%.1f%s%s" % (num, 'Yi', suffix)

def sizeOfPropagation(Ntot, dt, duration):
    """
    Estimates the size of the output from one THALASSA batch propagation.
    """
    NPoints = duration/dt * 365.25 + 1
    bytesEachProp = 2 * (602 + NPoints * 161)
    bytesTot = Ntot * bytesEachProp
    sizeTot = sizeof_fmt(bytesTot, suffix='B')
    return sizeTot

def runThalassa(outDir, resume, SID):
    """
    Launch THALASSA for the propagation identified by 'SID'.
    """
    # Get MPI rank for logging
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    hostname = socket.gethostname()
    
    # Reconstruct directory from SID
    iChunk = (SID - 1) // chunkSize + 1
    subDir = 'C{:03d}'.format(iChunk)
    subSubDir = 'S{:010d}'.format(int(SID))
    outfile = os.path.join(outDir, subDir, subSubDir, 'orbels.dat')

    if resume and os.path.isfile(outfile):
        print(f'[{hostname}:Rank{rank:03d}] Simulation SID {SID} already exists. Skipping...', flush=True)
        return 0  # Return 0 for skipped
    else:
        if resume:
            try:
                with open(os.path.join(outDir, subDir, subSubDir, 'input.txt'), 'r') as f:
                    inpFile = f.readlines()
                inpFile[44] = "out:   " + os.path.abspath(os.path.join(outDir, subDir, subSubDir, ' '))
                with open(os.path.join(outDir, subDir, subSubDir, 'input.txt'), 'w') as f:
                    f.writelines(inpFile)
            except Exception as e:
                print(f'[{hostname}:Rank{rank:03d}] Error updating input file for SID {SID}: {str(e)}', flush=True)
                return -4
        
        print(f'[{hostname}:Rank{rank:03d}] Launching simulation SID = {SID}', flush=True)
        
        try:
            # Launch THALASSA
            result = subprocess.run(
                [thalassaPath,
                 os.path.abspath(os.path.join(outDir, subDir, subSubDir, 'input.txt')),
                 os.path.abspath(os.path.join(outDir, subDir, subSubDir, 'object.txt'))],
                capture_output=True,
                text=True,
                timeout=86400  # Timeout after 24 hours
            )
            
            if result.returncode != 0:
                print(f'[{hostname}:Rank{rank:03d}] ERROR in SID {SID}: {result.stderr[:200]}', flush=True)
                return -1  # Return -1 for error
            return 1  # Return 1 for success
            
        except subprocess.TimeoutExpired:
            print(f'[{hostname}:Rank{rank:03d}] Timeout in SID {SID} (24h limit)', flush=True)
            return -2  # Return -2 for timeout
        except FileNotFoundError as e:
            print(f'[{hostname}:Rank{rank:03d}] File not found for SID {SID}: {str(e)}', flush=True)
            return -3
        except Exception as e:
            print(f'[{hostname}:Rank{rank:03d}] Exception in SID {SID}: {str(e)}', flush=True)
            return -3  # Return -3 for other exceptions

def distribute_tasks(total_tasks, num_workers, worker_id):
    """
    Distribute tasks among MPI workers using block distribution.
    Returns the list of task IDs for this worker.
    """
    tasks_per_worker = total_tasks // num_workers
    remainder = total_tasks % num_workers
    
    start_idx = worker_id * tasks_per_worker + min(worker_id, remainder)
    end_idx = start_idx + tasks_per_worker + (1 if worker_id < remainder else 0)
    
    tasks = list(range(start_idx + 1, end_idx + 1))  # SID starts from 1
    return tasks

def main_mpi():
    """
    Main function for MPI execution.
    """
    # Initialize MPI
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()
    hostname = socket.gethostname()
    
    args = None
    gridDefDict = None
    nTot = 0
    
    # Only rank 0 parses arguments and reads configuration
    if rank == 0:
        parser = argparse.ArgumentParser(description='Launch THALASSA propagations '
                      'over a grid of initial conditions using MPI.')
        
        parser.add_argument('outDir', nargs='?',
                          help='path to the output directory for the batch propagations')
        parser.add_argument('--force',
                          help="don't ask for user confirmation before starting the simulation.",
                          action='store_true')
        parser.add_argument('--resume',
                          help='resume previous simulation. This skips directories where the file "propagation.log" is present.',
                          action='store_true')
        
        if len(sys.argv) == 1:
            parser.print_help(sys.stderr)
            comm.Abort(1)
        
        args = parser.parse_args()
        
        # Read grid definition
        gridDefFile = os.path.join(args.outDir, 'griddef.json')
        
        print('=' * 70)
        print('THALASSA GRID PROPAGATION SCRIPT (MPI4Py Version)')
        print('=' * 70)
        print(f'Number of MPI processes: {size}')
        print(f'Reading grid definition from {os.path.abspath(gridDefFile)}...', end=" ")
        
        try:
            with open(gridDefFile, 'r') as f:
                gridDefDict = json.load(f)
            
            # Count total simulations
            sDirs = []
            for root, dirs, files in os.walk(args.outDir):
                sDirs.extend([d for d in dirs if d.startswith('S')])
            nTot = len(sDirs)
            
            print('Done.')
            print(f'Total simulations to run: {nTot}')
        except Exception as e:
            print(f'Error reading grid definition: {str(e)}', file=sys.stderr)
            comm.Abort(1)
    
    # Broadcast arguments and data to all processes
    args = comm.bcast(args, root=0)
    gridDefDict = comm.bcast(gridDefDict, root=0)
    nTot = comm.bcast(nTot, root=0)
    
    # Only rank 0 displays summary and asks for confirmation
    if rank == 0:
        dt = gridDefDict["Integration"]["Step"]
        duration = gridDefDict["Integration"]["Duration"]
        sizeTot = sizeOfPropagation(nTot, dt, duration)
        
        proceedMsg = f"""\n=== JOB SUMMARY ===
Total propagations: {nTot}
Duration per simulation: {duration:g} years
Time step: {dt:g} days
Estimated disk space: {sizeTot}
MPI processes: {size}
Nodes: Unknown (will be shown during execution)
===============================
Do you want to continue? (Y/N): """
        
        if not args.force:
            proceed = input(proceedMsg)
            if proceed.lower() != 'y':
                print("Aborted by user.")
                comm.Abort(0)
        else:
            print(f"\n=== JOB SUMMARY ===")
            print(f"Total propagations: {nTot}")
            print(f"Duration per simulation: {duration:g} years")
            print(f"Time step: {dt:g} days")
            print(f"Estimated disk space: {sizeTot}")
            print(f"MPI processes: {size}")
            print("Starting execution (force mode)...")
    
    # Synchronize before starting
    comm.Barrier()
    
    # Start timing
    start_time = time.time()
    
    if rank == 0:
        startTime = datetime.datetime.now()
        log_filename = os.path.join(args.outDir, f'grid_mpi_{startTime.strftime("%Y%m%d_%H%M%S")}.log')
        if args.resume:
            log = open(log_filename, 'a', 1)
            log.write('\n' + '='*60 + '\n')
            log.write(f'Resuming simulation at {startTime.isoformat()}\n')
        else:
            log = open(log_filename, 'w', 1)
            log.write('THALASSA GRID PROPAGATION - MPI4Py VERSION - LOG FILE\n')
            log.write('='*60 + '\n')
        
        log.write(f'Start time: {startTime.isoformat()}\n')
        log.write(f'MPI processes: {size}\n')
        log.write(f'Total propagations: {nTot}\n')
        log.write(f'Duration: {duration:g} years\n')
        log.write(f'Step size: {dt:g} days\n')
        log.write(f'Output directory: {os.path.abspath(args.outDir)}\n')
        log.write(f'Resume mode: {args.resume}\n')
        log.write('-'*60 + '\n')
        log.flush()
    
    # Distribute tasks among MPI processes
    my_tasks = distribute_tasks(nTot, size, rank)
    
    # Each process reports its tasks
    print(f'[{hostname}:Rank{rank:03d}] Assigned {len(my_tasks)} tasks', flush=True)
    
    # Process assigned tasks
    task_results = []
    for i, task_id in enumerate(my_tasks):
        task_start = time.time()
        result = runThalassa(args.outDir, args.resume, task_id)
        task_end = time.time()
        task_time = task_end - task_start
        
        task_results.append((task_id, result, task_time))
        
        # Progress report every 10 tasks or at the end
        if (i + 1) % 10 == 0 or i == len(my_tasks) - 1:
            print(f'[{hostname}:Rank{rank:03d}] Progress: {i+1}/{len(my_tasks)} '
                  f'({(i+1)/len(my_tasks)*100:.1f}%) - Current: SID {task_id}', flush=True)
    
    # Gather results from all processes
    all_results = comm.gather(task_results, root=0)
    
    # End timing
    end_time = time.time()
    total_time = end_time - start_time
    
    # Rank 0 writes final summary
    if rank == 0:
        endTime = datetime.datetime.now()
        gridDur = endTime - startTime
        
        # Process all results
        total_success = 0
        total_skipped = 0
        total_errors = 0
        task_times = []
        
        for proc_results in all_results:
            for task_id, result, task_time in proc_results:
                task_times.append(task_time)
                if result == 1:
                    total_success += 1
                elif result == 0:
                    total_skipped += 1
                else:
                    total_errors += 1
        
        # Calculate statistics
        if task_times:
            avg_time = sum(task_times) / len(task_times)
            max_time = max(task_times)
            min_time = min(task_times)
        else:
            avg_time = max_time = min_time = 0
        
        log.write('\n' + '='*60 + '\n')
        log.write('RESULTS SUMMARY\n')
        log.write('='*60 + '\n')
        log.write(f'Successful propagations: {total_success}\n')
        log.write(f'Skipped propagations: {total_skipped}\n')
        log.write(f'Failed propagations: {total_errors}\n')
        log.write(f'Total processed: {total_success + total_skipped + total_errors}\n')
        log.write(f'Expected total: {nTot}\n')
        log.write('-'*60 + '\n')
        log.write(f'Start time: {startTime.isoformat()}\n')
        log.write(f'End time: {endTime.isoformat()}\n')
        log.write(f'Total wall time: {gridDur} ({total_time:.1f} seconds)\n')
        log.write(f'Average time per task: {avg_time:.1f} seconds\n')
        log.write(f'Fastest task: {min_time:.1f} seconds\n')
        log.write(f'Slowest task: {max_time:.1f} seconds\n')
        log.write(f'Parallel efficiency: {nTot * avg_time / (size * total_time) * 100:.1f}%\n')
        log.write('='*60 + '\n')
        log.close()
        
        # Print summary to console
        print('\n' + '='*70)
        print('THALASSA MPI GRID PROPAGATION COMPLETE')
        print('='*70)
        print(f'Successful: {total_success:6d}')
        print(f'Skipped:    {total_skipped:6d}')
        print(f'Failed:     {total_errors:6d}')
        print(f'Total:      {nTot:6d}')
        print(f'Wall time:  {gridDur} ({total_time:.1f} seconds)')
        print(f'Avg/task:   {avg_time:.1f} seconds')
        print(f'Efficiency: {nTot * avg_time / (size * total_time) * 100:.1f}%')
        print(f'Log file:   {log_filename}')
        print('='*70)
    
    # Final barrier to ensure all processes complete
    comm.Barrier()
    
    if rank == 0:
        print(f"\nAll MPI processes completed. Exiting.")

if __name__ == '__main__':
    main_mpi()